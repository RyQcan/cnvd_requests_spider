# cnvd_spider
cnvd_spider

python3.5

写在前面的话：
. 本人是半路转行来做码农的，现在还是一个小小的coder，很多东西都不会，在努力学习中，这个项目也是目前公司的一个项目之一， 其中还有很多不足，暂时先这样用着吧，等有时间了再优化。。。。。

需要用到的包：
.requests .lxml .csv .time .random .collections .codecs .pymysql .datetime

项目情况：
. 某天项目大佬，忽然来了个需求，要我把XX安全信息漏洞平台上的漏洞信息都弄下来，说准备自己弄一个数据库，把数据内容通过邮件发送给需要客户，然后我就开始努力了，网上百度一大波，竟然发现，反爬虫措施做的还算可以，毕竟算是XX级的网络了。还是有点实力的，好了。先这样吧，开干了。。。。。

. 目标网站：http://www.cnvd.org.cn/flaw/list.htm

. 需要的数据字段：公开日期，危害级别，影响产品，漏洞描述，参考链接，漏洞解决方案，厂商补丁

大概的思路
. 拿到项目第一天，首先分析了一波，看看网站的反爬，验证一下，需要不需要用代理什么的 . 发现就是对请求控制的比较严一点，用了scrapy 框架去做，直接返回空数据（假的数据），哇，我一看，先检查代码， 代码没问题，再看看返回的数据在哪一个详情页，发现有数据，然后我用requests去请求，返回的数据有的时候有，有的时候又没有，这个就比较。。。。蛋蛋有点伤了，然后好吧，我又用request 试试，再设置延时操作，额，发现有数据了，而且比较稳。。。。。。 . 其实我后来发现，这个requests是同步操作，前面的请求还没结束，后面的详情页的请求又来了。。。。。 . 哎，发现学艺不精后，自己又去补了一下。。。。

. 然后开始努力的码代码了。。。。 - 实现爬虫的套路 - 准备url - 准备start_url - url地址规律不明显，总数不确定 - 通过代码提取下一页的url - xpath - 寻找url地址，部分参数在当前的响应中（比如，当前页码数和总的页码数在当前的响应中） - 准备url_list - 页码总数明确 - url地址规律明显

  - 发送请求，获取响应
    - 添加随机的User-Agent,反反爬虫
    - 添加随机的代理ip，反反爬虫
    - 在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie
    - cookie的处理可以使用session来解决
    - 准备一堆能用的cookie，组成cookie池
      - 如果不登录
        - 准备刚开始能够成功请求对方网站的cookie，即接收对方网站设置在response的cookie
        - 下一次请求的时候，使用之前的列表中的cookie来请求
      - 如果登录
        - 准备多个账号
        - 使用程序获取每个账号的cookie
        - 之后请求登录之后才能访问的网站随机的选择cookie

  - 提取数据
    - 确定数据的位置
      - 如果数据在当前的url地址中
        - 提取的是列表页的数据
          - 直接请求列表页的url地址，不用进入详情页
        - 提取的是详情页的数据
          - 1. 确定url
          - 2. 发送请求
          - 3. 提取数据
          - 4. 返回
      - 如果数据不在当前的url地址中
        - 在其他的响应中，寻找数据的位置
          - 1. 从network中从上往下找
          - 2. 使用chrome中的过滤条件，选择出了js,css,img之外的按钮
          - 3. 使用chrome的search all file，搜索数字和英文
    - 数据的提取
      - xpath,从html中提取整块的数据，先分组，之后每一组再提取
      - re，提取max_time,price,html中的json字符串
      - json
  - 保存
    - 保存在本地，text,json,csv
    - 保存在数据库
运行情况
. 到目前为止,运行还算稳定.就是每天到晚上12点以后,再用这个请求数据就会被封,断链接,好奇怪的样子.看来是我自己的反反爬虫没有做好, 还准备用多线程的,看来没戏了,封IP就划不来了,先用着吧,欢迎大佬们帮忙解决,谢谢
